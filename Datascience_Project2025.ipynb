{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johny85/World-of-Scripts/blob/master/Datascience_Project2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello\\! 👋 As your data science mentor, I'd be happy to help you with your AQI project. This is an excellent topic for a comparative analysis, and breaking it down into manageable steps is key. I'll provide you with a comprehensive, step-by-step guide with the necessary Python code, explanations, and guidance on best practices for each stage of your project.\n",
        "\n",
        "Let's begin with the first and most critical task: **Data Preprocessing**.\n",
        "\n",
        "-----\n",
        "\n",
        "### 1\\. Data Preprocessing 🧹\n",
        "\n",
        "Before any analysis can be performed, you must prepare your data. This involves loading the data, combining it, cleaning it, and ensuring it is in a usable format.\n",
        "\n",
        "#### **Guidance**\n",
        "\n",
        "  * **File Loading and Combining:** The most efficient way to handle multiple CSV files is to load them programmatically in a loop. I will provide code that iterates through your files, adds a `City` column to each, and then combines them into a single pandas `DataFrame`.\n",
        "  * **Column Standardization:** The column names in your datasets include special characters and units, such as `PM2.5 (µg/m³)` and `AT (°C)`. It is a best practice to standardize these names into a more machine-readable format (e.g., `pm2_5`, `temp_c`).\n",
        "  * **Handling Missing Values:** Your data snippets show missing values as blank spaces or `NA`. These should be replaced with `NaN` (Not a Number) for proper numerical operations. After that, you can choose a strategy for imputation. Given the time-series nature of your data, **forward or backward fill (`fillna(method='ffill')`)** or **interpolation (`interpolate()`)** are often better choices than simple mean or median imputation, as they preserve the temporal relationships in the data.\n",
        "  * **Data Aggregation:** Your data appears to be recorded on a sub-daily basis. For your analysis and modeling, aggregating the data to a **daily or hourly level** is essential. You can use pandas' `resample()` method for this.\n",
        "  * **Handling Duplicates:** Always check for and remove duplicate rows to ensure the integrity of your dataset.\n",
        "\n",
        "#### **Python Code**\n",
        "\n",
        "Here is the Python code to perform the initial data preprocessing. You can copy and paste this into your Jupyter notebook and run it. **Please note:** If you are running this in a local environment, you may need to adjust the file paths in the `file_list` to where your files are stored. The code below assumes all the files are in the same directory."
      ],
      "metadata": {
        "id": "BNkV159Bc2CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Step 1: Read all the CSV files and combine them into a single DataFrame.\n",
        "# You can use glob.glob to get all files ending with .csv in your current directory.\n",
        "# If you are in a specific subdirectory, you might need to adjust the path.\n",
        "file_list = glob.glob('*.csv')\n",
        "\n",
        "# If glob doesn't work, you can manually list your files as a fallback:\n",
        "# file_list = [\n",
        "#     'Anantapur.csv', 'Chittoor.csv', 'Kadapa.csv', 'Rajamahendravaram.csv',\n",
        "#     'Tirupati.csv', 'Vijayawada.csv', 'Visakhapatnam.csv', 'Amravati.csv'\n",
        "# ]\n",
        "\n",
        "# Initialize an empty list to store the DataFrames.\n",
        "df_list = []\n",
        "\n",
        "# Loop through each file, read it into a DataFrame, and add a 'City' column.\n",
        "for file in file_list:\n",
        "    city_name = file.replace('.csv', '')\n",
        "    df = pd.read_csv(file)\n",
        "    df['City'] = city_name\n",
        "    df_list.append(df)\n",
        "\n",
        "# Concatenate all the DataFrames into one.\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Step 2: Standardize column names.\n",
        "new_columns = {\n",
        "    'PM2.5 (µg/m³)': 'pm2_5',\n",
        "    'PM10 (µg/m³)': 'pm10',\n",
        "    'NO (µg/m³)': 'no',\n",
        "    'NO2 (µg/m³)': 'no2',\n",
        "    'NOx (ppb)': 'nox',\n",
        "    'NH3 (µg/m³)': 'nh3',\n",
        "    'SO2 (µg/m³)': 'so2',\n",
        "    'CO (mg/m³)': 'co',\n",
        "    'Ozone (µg/m³)': 'ozone',\n",
        "    'Benzene (µg/m³)': 'benzene',\n",
        "    'Toluene (µg/m³)': 'toluene',\n",
        "    'Xylene (µg/m³)': 'xylene',\n",
        "    'O Xylene (µg/m³)': 'o_xylene',\n",
        "    'Eth-Benzene (µg/m³)': 'eth_benzene',\n",
        "    'MP-Xylene (µg/m³)': 'mp_xylene',\n",
        "    'AT (°C)': 'temp_c',\n",
        "    'RH (%)': 'rh_percent',\n",
        "    'WS (m/s)': 'ws_m_s',\n",
        "    'WD (deg)': 'wd_deg',\n",
        "    'RF (mm)': 'rf_mm',\n",
        "    'TOT-RF (mm)': 'tot_rf_mm',\n",
        "    'SR (W/mt2)': 'sr_w_mt2',\n",
        "    'BP (mmHg)': 'bp_mmHg',\n",
        "    'VWS (m/s)': 'vws_m_s',\n",
        "    'Timestamp': 'timestamp'\n",
        "}\n",
        "combined_df.rename(columns=new_columns, inplace=True)\n",
        "\n",
        "# Step 3: Handle missing values and convert data types.\n",
        "# Replace common representations of missing data with pandas' NaN.\n",
        "combined_df.replace(['', 'NA'], pd.NA, inplace=True)\n",
        "\n",
        "# Convert 'timestamp' to datetime and set it as the index.\n",
        "combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])\n",
        "combined_df.set_index('timestamp', inplace=True)\n",
        "\n",
        "# Convert all relevant columns to numeric type, coercing errors to NaN.\n",
        "for col in combined_df.columns:\n",
        "    if col not in ['City']:\n",
        "        combined_df[col] = pd.to_numeric(combined_df[col], errors='coerce')\n",
        "\n",
        "# Step 4: Check for and drop duplicates.\n",
        "duplicates = combined_df.duplicated().sum()\n",
        "print(f\"Found {duplicates} duplicate rows.\")\n",
        "if duplicates > 0:\n",
        "    combined_df.drop_duplicates(inplace=True)\n",
        "    print(\"Dropped duplicate rows.\")\n",
        "\n",
        "# Step 5: Resample the data to a daily average.\n",
        "# This is a key step for your analysis and modeling.\n",
        "daily_df = combined_df.resample('D').mean()\n",
        "# Add the city information back after resampling.\n",
        "daily_df['City'] = combined_df['City'].iloc[0]\n",
        "\n",
        "# You can save this cleaned and resampled data to a new CSV file.\n",
        "daily_df.to_csv('cleaned_aqi_data.csv')\n",
        "\n",
        "print(\"\\n--- Preprocessing Complete ---\")\n",
        "print(\"Head of the cleaned and resampled DataFrame:\")\n",
        "print(daily_df.head())\n",
        "print(\"\\nDataFrame Info:\")\n",
        "daily_df.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "WVuRqaVgc2CM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### 2\\. Exploratory Data Analysis (EDA) 📊\n",
        "\n",
        "Once your data is clean, you can start exploring it to uncover patterns, trends, and anomalies.\n",
        "\n",
        "#### **Guidance**\n",
        "\n",
        "  * **Time-Series Plots:** The best visualization for a time-series is a line plot. You can use **`matplotlib`** or **`seaborn`** to plot pollutants like PM2.5 and PM10 over time. Make a separate plot for each city or use a faceted plot to compare them.\n",
        "  * **Seasonal Decomposition:** To analyze seasonal peaks, you can use `statsmodels.tsa.seasonal.seasonal_decompose`. This will break down your time series into three components: **trend**, **seasonality**, and **residuals**. This is an excellent way to quantify the yearly cycle of air pollution.\n",
        "  * **Cross-City Boxplots/Heatmaps:** To compare pollution levels across cities, a **boxplot** is an ideal choice. It will show the median, quartiles, and outliers for each city's PM2.5 and PM10 levels. A **heatmap** of a correlation matrix is perfect for visualizing the relationships between pollutants and meteorological drivers.\n",
        "\n",
        "#### **Python Code & Visualization Choices**"
      ],
      "metadata": {
        "id": "K4eN3uXLc2CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Re-load the cleaned data if you are starting a new notebook session\n",
        "# daily_df = pd.read_csv('cleaned_aqi_data.csv', index_col='timestamp', parse_dates=True)\n",
        "\n",
        "# --- Visualization 1: Time-series Plot for PM2.5 ---\n",
        "plt.figure(figsize=(15, 6))\n",
        "sns.lineplot(data=daily_df, x=daily_df.index, y='pm2_5')\n",
        "plt.title('PM2.5 Levels Over Time (Daily Average)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('PM2.5 (µg/m³)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 2: Seasonal Boxplots ---\n",
        "# Create a 'Month' column for seasonal analysis.\n",
        "daily_df['month'] = daily_df.index.month\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(x='month', y='pm2_5', data=daily_df)\n",
        "plt.title('Monthly PM2.5 Levels')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('PM2.5 (µg/m³)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Visualization 3: Correlation Heatmap ---\n",
        "# Select the columns for the heatmap.\n",
        "pollutants = ['pm2_5', 'pm10', 'no2', 'so2', 'ozone', 'co']\n",
        "meteorological = ['temp_c', 'rh_percent', 'ws_m_s', 'rf_mm']\n",
        "all_cols = pollutants + meteorological\n",
        "correlation_matrix = daily_df[all_cols].corr(method='pearson')\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Matrix of Pollutants and Meteorological Drivers')\n",
        "plt.show()\n",
        "\n",
        "# --- Seasonal Decomposition ---\n",
        "# Choose a city to decompose.\n",
        "city_data = daily_df[daily_df['City'] == 'Visakhapatnam']['pm2_5'].dropna()\n",
        "# The period parameter should be set to the frequency of your data's seasonality.\n",
        "# Since we are using daily data with yearly seasonality, the period is 365.\n",
        "decomposition = seasonal_decompose(city_data, model='additive', period=365)\n",
        "decomposition.plot()\n",
        "plt.suptitle('Seasonal Decomposition of PM2.5 for Visakhapatnam', y=1.02)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8Wk5qQbpc2CU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### 3\\. Analysis and Modeling 📈\n",
        "\n",
        "This is where you'll quantify the relationships and make predictions.\n",
        "\n",
        "#### **Guidance**\n",
        "\n",
        "  * **Correlation:** Use both **Pearson** and **Spearman** correlation coefficients. Pearson measures linear relationships, while Spearman measures monotonic (but not necessarily linear) relationships. This will help you understand if the relationship between pollutants and meteorological drivers is linear or more complex.\n",
        "  * **Regression Analysis:** A linear regression model is a great starting point to quantify the influence of meteorological factors. The R-squared value will tell you the percentage of the variance in a pollutant that can be explained by the meteorological variables.\n",
        "  * **ANN for Prediction:** For your 7-day forecast, an Artificial Neural Network (ANN) is an excellent choice. You will need to **scale your data** using `StandardScaler`, create a supervised learning dataset with lagged features, and then train the model using `scikit-learn` or `keras`.\n",
        "\n",
        "#### **Python Code**"
      ],
      "metadata": {
        "id": "Emn7LIbPc2CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# --- Part 1: Correlation Analysis ---\n",
        "# Example for PM2.5 and temperature\n",
        "pm25_temp_pearson = pearsonr(daily_df['pm2_5'].dropna(), daily_df['temp_c'].dropna())\n",
        "pm25_temp_spearman = spearmanr(daily_df['pm2_5'].dropna(), daily_df['temp_c'].dropna())\n",
        "print(f\"Pearson Correlation (PM2.5 vs Temp): r={pm25_temp_pearson.statistic:.2f}, p-value={pm25_temp_pearson.pvalue:.2e}\")\n",
        "print(f\"Spearman Correlation (PM2.5 vs Temp): rho={pm25_temp_spearman.statistic:.2f}, p-value={pm25_temp_spearman.pvalue:.2e}\")\n",
        "\n",
        "# --- Part 2: Regression Analysis (PM2.5 vs Meteorological Drivers) ---\n",
        "# Assuming you have filled the missing values and selected your features\n",
        "features = ['temp_c', 'rh_percent', 'ws_m_s', 'rf_mm']\n",
        "target = 'pm2_5'\n",
        "# Drop rows with any NaN values for the analysis\n",
        "regression_data = daily_df.dropna(subset=features + [target])\n",
        "X = regression_data[features]\n",
        "y = regression_data[target]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"\\nLinear Regression Model Performance:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r2:.2f}\") # This is your % variance explained!\n",
        "\n",
        "# --- Part 3: ANN for 7-day Prediction (conceptual outline) ---\n",
        "# NOTE: This is a high-level template. You will need to build the\n",
        "# time-series dataset with lagged features for a true forecast.\n",
        "# For simplicity, this example shows a basic ANN setup.\n",
        "# Scale the data before feeding it to the ANN.\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the ANN model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1)) # Output layer for a single value prediction\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "history = model.fit(X_train, y_train, epochs=50, validation_split=0.2, verbose=0)\n",
        "print(\"\\nANN Model Training Complete.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "0FepX6Ptc2CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "### 4\\. Interpretation and Deliverables 📝\n",
        "\n",
        "#### **Guidance**\n",
        "\n",
        "  * **Report:** Your report should be structured logically. Start with an **Abstract/Objectives**, move to **Methods** (data collection, preprocessing, modeling), then present **Key Findings** (your EDA and analysis results), and conclude with a **Discussion of Limitations** and **Future Work**.\n",
        "  * **Slide Deck:** For your presentation, focus on one key insight per slide. Use a strong visual and a concise title. For example:\n",
        "      * **Slide 1: Title** (Project Name, Your Name)\n",
        "      * **Slide 2: Objectives** (Briefly state your goals)\n",
        "      * **Slide 3: Data Overview** (Show a table or summary of the data)\n",
        "      * **Slide 4: Seasonal Trends** (Show a seasonal decomposition plot)\n",
        "      * **Slide 5: City-to-City Comparison** (Show a boxplot of PM2.5 levels)\n",
        "      * **Slide 6: Correlation Analysis** (Show the heatmap)\n",
        "      * **Slide 7: Regression Results** (Display the R-squared value)\n",
        "      * **Slide 8: Forecasting Model** (Briefly explain your ANN)\n",
        "      * **Slide 9: Key Findings & Conclusion** (Summarize your main insights)\n",
        "      * **Slide 10: Q\\&A** (Open the floor for questions)\n",
        "\n",
        "You have a solid plan and the tools you need to get started. Remember to document your steps, as this will make writing your final report much easier. Good luck with your project\\! You've got this\\! ✨"
      ],
      "metadata": {
        "id": "YPJZffNlc2CY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}